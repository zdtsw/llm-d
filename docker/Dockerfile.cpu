# Using Ubuntu 22.04 to match upstream vLLM compilation environment with glibc 2.35 for now.
FROM ubuntu:22.04 AS base-common

ARG PYTHON_VERSION=3.12
ARG PIP_EXTRA_INDEX_URL="https://download.pytorch.org/whl/cpu/"
ARG VLLM_REPO="https://github.com/vllm-project/vllm.git"
# Pin to a commit SHA1/Tag
ARG VLLM_COMMIT=v0.14.0
ARG TARGETARCH
ARG TARGETOS=ubuntu
ARG ACCELERATOR=cpu
# Support for building with non-AVX512 vLLM
ARG VLLM_CPU_DISABLE_AVX512=0
# Support for building with AVX512BF16 ISA
ARG VLLM_CPU_AVX512BF16=0
# Support for building with AVX512VNNI ISA
ARG VLLM_CPU_AVX512VNNI=0

# Install base packages using manifest system
COPY docker/scripts/common/package-utils.sh /tmp/package-utils.sh
COPY docker/packages /tmp/packages
COPY docker/scripts/common/builder/install-builder-packages.sh /tmp/install-builder-packages.sh
RUN --mount=type=cache,target=/var/cache/apt,sharing=locked \
    --mount=type=cache,target=/var/lib/apt,sharing=locked \
    TARGETOS=${TARGETOS} ACCELERATOR=${ACCELERATOR} /tmp/install-builder-packages.sh && \
    rm -f /tmp/install-builder-packages.sh /tmp/package-utils.sh && \
    rm -rf /tmp/packages

# Update certificates, configure gcc-12 as default, and install uv
RUN update-ca-certificates && \
    update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-12 10 --slave /usr/bin/g++ g++ /usr/bin/g++-12 && \
    curl -LsSf https://astral.sh/uv/install.sh | sh

ENV CCACHE_DIR=/root/.cache/ccache
ENV CMAKE_CXX_COMPILER_LAUNCHER=ccache

ENV PATH="/root/.local/bin:$PATH"
ENV VIRTUAL_ENV="/opt/venv"
ENV UV_PYTHON_INSTALL_DIR=/opt/uv/python
RUN uv venv --python ${PYTHON_VERSION} --seed ${VIRTUAL_ENV}

ENV PATH="$VIRTUAL_ENV/bin:$PATH"

# Clone vLLM source code
WORKDIR /workspace
RUN --mount=type=cache,target=/var/cache/git \
    git clone ${VLLM_REPO} vllm && \
    cd vllm && \
    git checkout ${VLLM_COMMIT}

# Install Python dependencies
WORKDIR /workspace/vllm
ENV PIP_EXTRA_INDEX_URL=${PIP_EXTRA_INDEX_URL}
ENV UV_EXTRA_INDEX_URL=${PIP_EXTRA_INDEX_URL}
ENV UV_INDEX_STRATEGY="unsafe-best-match"
ENV UV_LINK_MODE="copy"
ENV UV_HTTP_TIMEOUT=500
ENV UV_HTTP_RETRIES=10
RUN --mount=type=cache,target=/root/.cache/uv \
    uv pip install --upgrade pip && \
    uv pip install -r requirements/cpu.txt -v

RUN echo 'ulimit -c 0' >> ~/.bashrc

ENV TARGETARCH=${TARGETARCH}

# Only for runtime consistency
ENV VLLM_CPU_DISABLE_AVX512=${VLLM_CPU_DISABLE_AVX512}
ENV VLLM_CPU_AVX512BF16=${VLLM_CPU_AVX512BF16}
ENV VLLM_CPU_AVX512VNNI=${VLLM_CPU_AVX512VNNI}

######################### x86_64 BASE IMAGE #########################
FROM base-common AS base-amd64

ENV LD_PRELOAD="/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4:/opt/venv/lib/libiomp5.so"

######################### arm64 BASE IMAGE #########################
FROM base-common AS base-arm64

ENV LD_PRELOAD="/usr/lib/aarch64-linux-gnu/libtcmalloc_minimal.so.4"

######################### BUILD IMAGE #########################
# Build vLLM wheel from source
###############################################################
FROM base-${TARGETARCH} AS vllm-build

# Reduce parallel job from 32(vllm prev-0.13.0 pytorch2.8) to 6 due to vllm v0.14.0(pytorch2.9+)
# resource requirements
# Lower value to prevent runner OOM/timeout during wheel build
ARG max_jobs=6

WORKDIR /workspace/vllm

RUN --mount=type=cache,target=/root/.cache/uv \
   uv pip install -r requirements/cpu-build.txt

RUN --mount=type=cache,target=/root/.cache/uv \
    --mount=type=cache,target=/root/.cache/ccache \
    --mount=type=cache,target=/workspace/vllm/.deps,sharing=locked \
    MAX_JOBS=${max_jobs} VLLM_TARGET_DEVICE=cpu \
    python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38

######################### RUNTIME IMAGE #########################
# Final runtime image with install vLLM wheel + NIXL + UCX
#################################################################
FROM vllm-build

ARG TARGETOS=ubuntu
ARG ACCELERATOR=cpu
# These env variables are set to satisfy lint on dockerfile
ARG PYTHON_VERSION=3.12
ARG CUDA_MAJOR
ARG CUDA_MINOR
ARG TARGETPLATFORM

ENV LD_LIBRARY_PATH="/usr/local/lib:/usr/lib:${LD_LIBRARY_PATH}"

RUN --mount=type=bind,from=vllm-build,src=/workspace/vllm/dist,target=dist \
    pip install dist/*.whl

# Install runtime packages using manifest system
COPY docker/scripts/common/package-utils.sh /tmp/package-utils.sh
COPY docker/packages /tmp/packages
COPY docker/scripts/common/runtime/install-runtime-packages.sh /tmp/install-runtime-packages.sh
RUN TARGETOS=${TARGETOS} ACCELERATOR=${ACCELERATOR} /tmp/install-runtime-packages.sh && \
    rm -f /tmp/install-runtime-packages.sh /tmp/package-utils.sh && \
    rm -rf /tmp/packages

# Copy install_nixl.py script
COPY docker/scripts/cpu/install_nixl.py /workspace/install_nixl.py

# Install nixl + UCX
RUN python3 /workspace/install_nixl.py

WORKDIR /workspace
ENTRYPOINT ["python", "-m", "vllm.entrypoints.openai.api_server"]
