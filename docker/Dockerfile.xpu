ARG ONEAPI_VERSION=2025.2.2-0
ARG VLLM_REPO="https://github.com/vllm-project/vllm.git"
ARG VLLM_COMMIT_SHA="72506c98349d6bcd32b4e33eec7b5513453c1502"

FROM intel/deep-learning-essentials:${ONEAPI_VERSION}-devel-ubuntu24.04 AS vllm-base

ARG CACHE_BUSTER
ARG PYTHON_VERSION=3.12
ARG TARGETOS=ubuntu
ARG ACCELERATOR=xpu
# These env variables are set to satisfy lint on dockerfile
ARG CUDA_MAJOR
ARG CUDA_MINOR
ARG TARGETPLATFORM

RUN if [ -n "${CACHE_BUSTER}" ]; then \
        echo "$CACHE_BUSTER" > /tmp/builder-buster; \
    fi;

SHELL ["/bin/bash", "-o", "pipefail", "-c"]

# Setup Intel repositories
RUN wget -O- https://apt.repos.intel.com/intel-gpg-keys/GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB | gpg --dearmor | tee /usr/share/keyrings/oneapi-archive-keyring.gpg > /dev/null && \
    echo "deb [signed-by=/usr/share/keyrings/oneapi-archive-keyring.gpg] https://apt.repos.intel.com/oneapi all main" | tee /etc/apt/sources.list.d/oneAPI.list && \
    add-apt-repository -y ppa:kobuk-team/intel-graphics

# Install base packages using manifest system
COPY docker/scripts/common/package-utils.sh /tmp/package-utils.sh
COPY docker/packages /tmp/packages
COPY docker/scripts/common/builder/install-builder-packages.sh /tmp/install-builder-packages.sh
RUN TARGETOS=${TARGETOS} ACCELERATOR=${ACCELERATOR} PYTHON_VERSION=${PYTHON_VERSION} /tmp/install-builder-packages.sh && \
    rm -f /tmp/install-builder-packages.sh /tmp/package-utils.sh && \
    rm -rf /tmp/packages

# Configure Python alternatives
RUN update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.12 1 && \
    update-alternatives --install /usr/bin/python python /usr/bin/python3.12 1

# This oneccl contains the BMG support which is not the case for default version of oneapi 2025.2.
RUN wget https://github.com/uxlfoundation/oneCCL/releases/download/2021.15.6/intel-oneccl-2021.15.6.9_offline.sh
RUN bash intel-oneccl-2021.15.6.9_offline.sh -a --silent --eula accept && \
    echo "source /opt/intel/oneapi/setvars.sh --force" >> /root/.bashrc && \
    echo "source /opt/intel/oneapi/ccl/2021.15/env/vars.sh --force" >> /root/.bashrc

SHELL ["/bin/bash", "-c"]
CMD ["/bin/bash", "-c", "source /root/.bashrc && exec bash"]

# Clone vLLM and build for XPU
WORKDIR /workspace

ARG VLLM_REPO
ARG VLLM_COMMIT_SHA
RUN --mount=type=cache,target=/var/cache/git \
    git clone ${VLLM_REPO} vllm && \
    cd vllm && \
    git checkout ${VLLM_COMMIT_SHA}


WORKDIR /workspace/vllm

# suppress the python externally managed environment error
RUN python3 -m pip config set global.break-system-packages true

RUN --mount=type=cache,target=/root/.cache/pip \
    pip install --no-cache-dir \
    -r requirements/xpu.txt

ENV LD_LIBRARY_PATH="$LD_LIBRARY_PATH:/usr/local/lib/"

ENV VLLM_TARGET_DEVICE=xpu
ENV VLLM_WORKER_MULTIPROC_METHOD=spawn

RUN --mount=type=cache,target=/root/.cache/pip \
    pip install --no-build-isolation .

CMD ["/bin/bash"]

FROM vllm-base AS runtime

# install additional dependencies for openai api server
# hadolint ignore=DL3013
RUN --mount=type=cache,target=/root/.cache/pip \
    pip install accelerate hf_transfer pytest pytest_asyncio lm_eval[api] modelscope

# install development dependencies (for testing)
RUN python3 -m pip install -e tests/vllm_test_utils

# install nixl from source code using a compatible version
COPY docker/scripts/xpu/install_nixl_from_source_ubuntu.py /tmp/install_nixl_from_source_ubuntu.py
# Pin NIXL_VERSION for reproducible builds and to ensure compatibility with the current
# vLLM XPU runtime; the installer script can fetch the latest version if this is unset,
# but we intentionally use a fixed, tested version here.
ENV NIXL_VERSION=0.7.0
RUN python3 /tmp/install_nixl_from_source_ubuntu.py

# PyJWT-2.7.0 will influence some wheel behaviors, remove its dist-info to avoid conflicts
RUN rm /usr/lib/python3/dist-packages/PyJWT-2.7.0.dist-info/ -rf

# remove torch bundled oneccl to avoid conflicts
RUN --mount=type=cache,target=/root/.cache/pip \
    pip uninstall oneccl oneccl-devel -y

ENTRYPOINT ["python", "-m", "vllm.entrypoints.openai.api_server"]
