name: Nightly - Inference Scheduling E2E (OpenShift)

# Nightly regression test for the inference-scheduling guide on OpenShift.
# Deploys via helmfile and validates with e2e-validate.sh.

on:
  schedule:
    - cron: '0 0 * * *'  # Midnight UTC daily
  workflow_dispatch:
    inputs:
      helmfile_env:
        description: 'Helmfile environment'
        required: false
        default: 'istio'
        type: choice
        options:
          - istio
          - kgateway
      skip_cleanup:
        description: 'Skip cleanup after tests (for debugging)'
        required: false
        default: 'false'

permissions:
  contents: read

concurrency:
  group: nightly-e2e-inference-scheduling
  cancel-in-progress: true

jobs:
  nightly:
    if: github.repository == 'llm-d/llm-d'
    uses: llm-d/llm-d-infra/.github/workflows/reusable-nightly-e2e-openshift-helmfile.yaml@main
    with:
      guide_name: inference-scheduling
      namespace: llm-d-nightly-inference
      helmfile_env: ${{ github.event.inputs.helmfile_env || 'istio' }}
      gateway_type: ${{ github.event.inputs.helmfile_env || 'istio' }}
      accelerator_type: H100
      required_gpus: 16
      recommended_gpus: 16
      pod_wait_timeout: '60m'
      pod_readiness_delay: 180
      image_override: 'ghcr.io/llm-d/llm-d-cuda-dev:latest'
      allow_gpu_preemption: true
      install_gateway_provider: false
      skip_cleanup: ${{ github.event.inputs.skip_cleanup == 'true' }}
      pre_deploy_script: |
        echo "Adding H100 nodeSelector for OCP deployment..."
        yq e '.decode.extraConfig.nodeSelector["nvidia.com/gpu.product"] = "NVIDIA-H100-80GB-HBM3"' -i "${GUIDE_PATH}/ms-inference-scheduling/values.yaml"
        echo "Updated values.yaml with H100 nodeSelector"
        yq e '.decode.extraConfig.nodeSelector' "${GUIDE_PATH}/ms-inference-scheduling/values.yaml"
    secrets: inherit
