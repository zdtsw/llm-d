# 
# This an example guide that leverages the guidellm harness to demonstrates Workload Variant Autoscaler (WVA)
# From running this experiment, it will be observed that decode pod replicas will autoscale as traffic rate increases 
# and decreases over time.
# 
# WVA currently only supports the Inference Scheduling well-lit-path.
# 
# To see scale up and scale down occur while this workload is running
# follow the below steps...
# 
# # In one terminal, in the namespace containing the llm-d stack, run:
# > kubectl get hpa -w
# 
# # In another terminal, in the namespace containing the llm-d stack, run:
# > kubectl get po -w
# 
# After sufficient work spawned, you will observe HPA replica targets increase
# as well as decode pods increase. Once workload has decreased sufficiently, 
# the replicas for decode pods will scale back down.
# 


endpoint:
  stack_name: &stack_name inference-scheduling-Llama-3.1-8B  # user defined name for the stack (results prefix)
  model: &model meta-llama/Llama-3.1-8B       # Exact HuggingFace model name. Must match stack deployed.
  namespace: &namespace ${NAMESPACE}  # Namespace where stack is deployed
  base_url: &url http://${GATEWAY_SVC}.${NAMESPACE}.svc.cluster.local:80  # Base URL of inference endpoint
  hf_token_secret: llm-d-hf-token     # The name of secret that contains the HF token of the stack

control:
  work_dir: $HOME/llm-d-bench-work  # working directory to store temporary and autogenerated files.
                                    # Do not edit content manually.
                                    # If not set, a temp directory will be created.
  kubectl: kubectl                  # kubectl command: kubectl or oc

harness:
  name: &harness_name guidellm
  results_pvc: ${BENCHMARK_PVC}   # PVC where benchmark results are stored
  namespace: *namespace           # Namespace where harness is deployed. Typically with stack.
  parallelism: 1                  # Number of parallel workload launcher pods to create.
  wait_timeout: 6000              # Time (in seconds) to wait for workload launcher pod to complete before terminating.
                                  # Set to 0 to disable timeout.
  image: ghcr.io/llm-d/llm-d-benchmark:v0.4.9
  # dataset_url: &dataset_url none

workload:                         # yaml configuration for harness workload(s)

  # Example of a simple guidellm workload
  rate_comparison:
    target: *url
    model: *model
    request_type: text_completions
    profile: constant
    rate: [4,8,16,24]
    max_seconds: 300
    data:
      prompt_tokens_min: 10
      prompt_tokens_max: 8192
      prompt_tokens: 4096
      prompt_tokens_stdev: 2048
      output_tokens_min: 10
      output_tokens_max: 2048
      output_tokens: 1024
      output_tokens_stdev: 512
      samples: 1000
