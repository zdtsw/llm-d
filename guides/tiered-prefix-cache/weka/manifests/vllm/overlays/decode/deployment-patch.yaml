apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-d-model-server
spec:
  replicas: 1
  selector:
    matchLabels:
      llm-d.ai/role: decode
  template:
    metadata:
      labels:
        llm-d.ai/role: decode
    spec:
      serviceAccountName: weka-vllm
      initContainers:
        - name: routing-proxy
          image: ghcr.io/llm-d/llm-d-routing-sidecar:v0.5.0
          imagePullPolicy: Always
          args:
            - --port=8000
            - --vllm-port=8200
            - --connector=nixlv2
            - -v=1
            - --secure-proxy=false
          ports:
            - containerPort: 8000
          resources: {}
          restartPolicy: Always
          securityContext:
            allowPrivilegeEscalation: false
            runAsNonRoot: true
        - name: create-cufile-on-node
          image: quay.io/grpereir/amg-utils:latest
          command:
            - /usr/local/bin/entrypoint.sh
          args:
            - /bin/bash
            - -lc
            - "true"
      containers:
        - name: vllm
          args:
            - RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
            - --port
            - "8200"
            - --served-model-name
            - RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
            - --tensor-parallel-size
            - "4"
            - --block-size
            - "128"
            - --kv-transfer-config
            - '{"kv_connector":"NixlConnector", "kv_role":"kv_both"}'
            - --disable-log-requests
            - --disable-uvicorn-access-log
            - --max-model-len
            - "32000"
          readinessProbe:
            httpGet:
              path: /v1/models
              port: 8200
            periodSeconds: 5
            timeoutSeconds: 2
            failureThreshold: 3
          livenessProbe:
            httpGet:
              path: /health
              port: 8200
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
          resources:
            limits:
              cpu: "16"
              memory: 64Gi
              nvidia.com/gpu: "4"
              rdma/ib: 1
            requests:
              cpu: "16"
              memory: 64Gi
              nvidia.com/gpu: "4"
              rdma/ib: 1
